2025-04-18 11:59:51,410 | INFO | Experiment logger initialized for experiment_exp_1. Log file: artifacts\highway-ppo\logs\20250418_115951_experiment_exp_1.log
2025-04-18 11:59:51,426 | INFO | 
=== Starting Experiment 2/162: feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64 ===

2025-04-18 11:59:51,426 | INFO | State dimension: 60, Action dimension: 2
2025-04-18 11:59:51,426 | INFO | Using features: ['x', 'y', 'vx', 'vy']
2025-04-18 11:59:53,537 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Starting training for experiment: feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64
2025-04-18 11:59:53,538 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Device: GPU: NVIDIA GeForce RTX 4070 Laptop GPU
2025-04-18 11:59:53,538 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Max episodes: 1500, Target reward: 20.0
2025-04-18 11:59:53,538 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Environment: highway-v0
2025-04-18 11:59:53,538 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Steps per update: 2048, PPO epochs: 4
2025-04-18 11:59:53,538 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Learning rate: 0.0001, Gamma: 0.99, Lambda: 0.95
2025-04-18 11:59:53,538 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Clip epsilon: 0.2, Value coef: 0.5, Entropy coef: 0.005
2025-04-18 11:59:53,538 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Performing initial evaluation...
2025-04-18 12:01:01,569 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] initial_eval reward=21.68
2025-04-18 12:02:41,732 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=10 reward=0.82 avg_reward=5.83 steps=1628 episode_steps=200 time=168.19s
2025-04-18 12:03:08,179 | INFO | update_complete loss=0.2893 policy_loss=-0.0011 value_loss=0.6092 entropy=2.8367 clip_frac=0.000 kl=0.00039 explained_var=-0.002
2025-04-18 12:04:33,933 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=20 reward=5.26 avg_reward=5.12 steps=3448 episode_steps=200 time=280.39s
2025-04-18 12:05:13,790 | INFO | update_complete loss=0.1903 policy_loss=-0.0012 value_loss=0.4113 entropy=2.8356 clip_frac=0.001 kl=0.00036 explained_var=0.037
2025-04-18 12:06:15,655 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=30 reward=8.96 avg_reward=6.05 steps=5096 episode_steps=200 time=382.12s
2025-04-18 12:07:20,421 | INFO | update_complete loss=0.4693 policy_loss=-0.0003 value_loss=0.9676 entropy=2.8372 clip_frac=0.000 kl=0.00017 explained_var=0.070
2025-04-18 12:07:35,604 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=40 reward=4.14 avg_reward=6.14 steps=6393 episode_steps=7 time=462.07s
2025-04-18 12:09:22,722 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=50 reward=0.73 avg_reward=5.86 steps=8050 episode_steps=200 time=569.18s
2025-04-18 12:09:22,722 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Evaluating at episode 50...
2025-04-18 12:09:48,322 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] eval episode=50 reward=9.38 avg_reward=15.53 time=594.78s
2025-04-18 12:09:48,328 | INFO | model_saved path=artifacts\highway-ppo\ppo_highway_best_feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64.pth
2025-04-18 12:09:48,328 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] New best model saved with average reward: 15.53
2025-04-18 12:09:57,827 | INFO | update_complete loss=0.2927 policy_loss=-0.0007 value_loss=0.6152 entropy=2.8372 clip_frac=0.001 kl=0.00031 explained_var=0.296
2025-04-18 12:12:13,250 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=60 reward=2.28 avg_reward=5.74 steps=9992 episode_steps=200 time=739.71s
2025-04-18 12:12:28,492 | INFO | update_complete loss=0.3485 policy_loss=-0.0016 value_loss=0.7286 entropy=2.8370 clip_frac=0.003 kl=0.00051 explained_var=0.307
2025-04-18 12:13:44,376 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=70 reward=6.73 avg_reward=6.33 steps=11471 episode_steps=200 time=830.84s
2025-04-18 12:14:34,579 | INFO | update_complete loss=0.6657 policy_loss=-0.0016 value_loss=1.3631 entropy=2.8388 clip_frac=0.012 kl=0.00114 explained_var=0.285
2025-04-18 12:15:22,982 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=80 reward=20.58 avg_reward=11.16 steps=13096 episode_steps=200 time=929.44s
2025-04-18 12:16:39,030 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=90 reward=4.49 avg_reward=7.17 steps=14336 episode_steps=154 time=1005.49s
2025-04-18 12:16:39,774 | INFO | update_complete loss=0.7087 policy_loss=-0.0013 value_loss=1.4484 entropy=2.8372 clip_frac=0.000 kl=0.00028 explained_var=0.394
2025-04-18 12:18:46,899 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=100 reward=2.94 avg_reward=7.96 steps=16160 episode_steps=200 time=1133.36s
2025-04-18 12:18:46,899 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Evaluating at episode 100...
2025-04-18 12:18:51,226 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] eval episode=100 reward=11.90 avg_reward=14.32 time=1137.69s
2025-04-18 12:19:07,439 | INFO | update_complete loss=0.2919 policy_loss=-0.0011 value_loss=0.6144 entropy=2.8361 clip_frac=0.003 kl=0.00047 explained_var=0.505
2025-04-18 12:20:37,058 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=110 reward=18.76 avg_reward=7.95 steps=17657 episode_steps=200 time=1243.52s
2025-04-18 12:21:31,029 | INFO | update_complete loss=0.5725 policy_loss=-0.0036 value_loss=1.1805 entropy=2.8350 clip_frac=0.002 kl=0.00040 explained_var=0.496
2025-04-18 12:22:14,433 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=120 reward=8.94 avg_reward=5.54 steps=19079 episode_steps=200 time=1340.90s
2025-04-18 12:23:42,442 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=130 reward=7.21 avg_reward=10.97 steps=20343 episode_steps=200 time=1428.90s
2025-04-18 12:23:53,170 | INFO | update_complete loss=0.7080 policy_loss=-0.0033 value_loss=1.4510 entropy=2.8347 clip_frac=0.015 kl=0.00162 explained_var=0.381
2025-04-18 12:25:43,372 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=140 reward=20.82 avg_reward=8.65 steps=22080 episode_steps=200 time=1549.83s
2025-04-18 12:26:15,167 | INFO | update_complete loss=0.6857 policy_loss=-0.0038 value_loss=1.4072 entropy=2.8335 clip_frac=0.005 kl=0.00071 explained_var=0.579
2025-04-18 12:26:58,723 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=150 reward=2.90 avg_reward=7.41 steps=23176 episode_steps=200 time=1625.18s
2025-04-18 12:26:58,723 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Evaluating at episode 150...
2025-04-18 12:27:45,493 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] eval episode=150 reward=5.35 avg_reward=12.08 time=1671.95s
2025-04-18 12:29:17,227 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=160 reward=5.65 avg_reward=6.22 steps=24462 episode_steps=24 time=1763.69s
2025-04-18 12:29:25,940 | INFO | update_complete loss=0.4730 policy_loss=-0.0021 value_loss=0.9787 entropy=2.8330 clip_frac=0.004 kl=0.00076 explained_var=0.569
2025-04-18 12:30:26,193 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=170 reward=7.49 avg_reward=13.28 steps=25485 episode_steps=20 time=1832.65s
2025-04-18 12:31:45,857 | INFO | update_complete loss=1.0983 policy_loss=-0.0019 value_loss=2.2289 entropy=2.8320 clip_frac=0.002 kl=0.00059 explained_var=0.502
2025-04-18 12:32:00,165 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=180 reward=7.89 avg_reward=8.26 steps=26824 episode_steps=200 time=1926.63s
2025-04-18 12:33:09,823 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=190 reward=2.27 avg_reward=8.65 steps=27837 episode_steps=18 time=1996.29s
2025-04-18 12:33:46,777 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=200 reward=2.20 avg_reward=9.11 steps=28390 episode_steps=11 time=2033.24s
2025-04-18 12:33:46,777 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Evaluating at episode 200...
2025-04-18 12:33:54,069 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] eval episode=200 reward=17.21 avg_reward=13.11 time=2040.53s
2025-04-18 12:34:13,734 | INFO | update_complete loss=0.9019 policy_loss=-0.0028 value_loss=1.8376 entropy=2.8319 clip_frac=0.009 kl=0.00135 explained_var=0.432
2025-04-18 12:34:50,095 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=210 reward=1.74 avg_reward=7.56 steps=29218 episode_steps=18 time=2096.56s
2025-04-18 12:35:49,399 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=220 reward=4.92 avg_reward=11.04 steps=30036 episode_steps=27 time=2155.86s
2025-04-18 12:36:37,249 | INFO | update_complete loss=1.5591 policy_loss=-0.0065 value_loss=3.1595 entropy=2.8316 clip_frac=0.048 kl=0.00388 explained_var=0.465
2025-04-18 12:37:08,589 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=230 reward=7.20 avg_reward=14.01 steps=31178 episode_steps=15 time=2235.05s
2025-04-18 12:38:03,985 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=240 reward=15.80 avg_reward=11.38 steps=32008 episode_steps=200 time=2290.45s
2025-04-18 12:38:53,210 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=250 reward=44.71 avg_reward=12.84 steps=32762 episode_steps=200 time=2339.67s
2025-04-18 12:38:53,210 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] Evaluating at episode 250...
2025-04-18 12:39:13,829 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] eval episode=250 reward=41.14 avg_reward=17.78 time=2360.29s
2025-04-18 12:39:13,837 | INFO | model_saved path=artifacts\highway-ppo\ppo_highway_best_feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64.pth
2025-04-18 12:39:13,837 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] New best model saved with average reward: 17.78
2025-04-18 12:39:15,462 | INFO | update_complete loss=2.6516 policy_loss=-0.0008 value_loss=5.3330 entropy=2.8303 clip_frac=0.002 kl=0.00088 explained_var=0.476
2025-04-18 12:40:03,228 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=260 reward=5.92 avg_reward=7.54 steps=33430 episode_steps=30 time=2409.69s
2025-04-18 12:40:56,688 | INFO | [feat=x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=64] episode=270 reward=8.82 avg_reward=8.47 steps=34215 episode_steps=14 time=2463.15s
