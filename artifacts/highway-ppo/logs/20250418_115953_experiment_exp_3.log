2025-04-18 11:59:53,086 | INFO | Experiment logger initialized for experiment_exp_3. Log file: artifacts\highway-ppo\logs\20250418_115953_experiment_exp_3.log
2025-04-18 11:59:53,105 | INFO | 
=== Starting Experiment 4/162: feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32 ===

2025-04-18 11:59:53,105 | INFO | State dimension: 75, Action dimension: 2
2025-04-18 11:59:53,105 | INFO | Using features: ['presence', 'x', 'y', 'vx', 'vy']
2025-04-18 11:59:54,666 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Starting training for experiment: feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32
2025-04-18 11:59:54,667 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Device: GPU: NVIDIA GeForce RTX 4070 Laptop GPU
2025-04-18 11:59:54,667 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Max episodes: 1500, Target reward: 20.0
2025-04-18 11:59:54,667 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Environment: highway-v0
2025-04-18 11:59:54,667 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Steps per update: 2048, PPO epochs: 4
2025-04-18 11:59:54,667 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Learning rate: 0.0001, Gamma: 0.99, Lambda: 0.95
2025-04-18 11:59:54,667 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Clip epsilon: 0.2, Value coef: 0.5, Entropy coef: 0.005
2025-04-18 11:59:54,667 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Performing initial evaluation...
2025-04-18 12:00:49,925 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] initial_eval reward=4.24
2025-04-18 12:02:42,575 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=10 reward=13.90 avg_reward=11.09 steps=1808 episode_steps=200 time=167.91s
2025-04-18 12:02:59,072 | INFO | update_complete loss=0.8133 policy_loss=-0.0003 value_loss=1.6556 entropy=2.8367 clip_frac=0.005 kl=0.00083 explained_var=-0.005
2025-04-18 12:04:25,021 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=20 reward=20.08 avg_reward=9.06 steps=3459 episode_steps=200 time=270.35s
2025-04-18 12:05:05,666 | INFO | update_complete loss=0.5672 policy_loss=-0.0011 value_loss=1.1649 entropy=2.8338 clip_frac=0.017 kl=0.00197 explained_var=0.147
2025-04-18 12:06:07,549 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=30 reward=11.86 avg_reward=7.02 steps=5117 episode_steps=200 time=372.88s
2025-04-18 12:07:14,075 | INFO | update_complete loss=0.8140 policy_loss=-0.0026 value_loss=1.6616 entropy=2.8328 clip_frac=0.032 kl=0.00270 explained_var=0.237
2025-04-18 12:07:52,813 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=40 reward=7.18 avg_reward=8.87 steps=6759 episode_steps=200 time=478.15s
2025-04-18 12:09:29,015 | INFO | update_complete loss=0.3324 policy_loss=-0.0065 value_loss=0.7062 entropy=2.8304 clip_frac=0.029 kl=0.00275 explained_var=0.491
2025-04-18 12:09:53,302 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=50 reward=6.47 avg_reward=7.79 steps=8592 episode_steps=200 time=598.63s
2025-04-18 12:09:53,302 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Evaluating at episode 50...
2025-04-18 12:11:09,661 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] eval episode=50 reward=20.75 avg_reward=12.50 time=674.99s
2025-04-18 12:11:09,668 | INFO | model_saved path=artifacts\highway-ppo\ppo_highway_best_feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32.pth
2025-04-18 12:11:09,668 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] New best model saved with average reward: 12.50
2025-04-18 12:13:01,119 | INFO | update_complete loss=0.3557 policy_loss=-0.0016 value_loss=0.7428 entropy=2.8273 clip_frac=0.010 kl=0.00117 explained_var=0.553
2025-04-18 12:13:13,830 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=60 reward=5.28 avg_reward=7.46 steps=10440 episode_steps=200 time=799.16s
2025-04-18 12:15:05,103 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=70 reward=9.91 avg_reward=11.20 steps=12254 episode_steps=200 time=910.44s
2025-04-18 12:15:08,440 | INFO | update_complete loss=0.6734 policy_loss=-0.0021 value_loss=1.3793 entropy=2.8286 clip_frac=0.015 kl=0.00204 explained_var=0.374
2025-04-18 12:17:04,643 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=80 reward=17.22 avg_reward=16.60 steps=14088 episode_steps=200 time=1029.98s
2025-04-18 12:17:23,384 | INFO | update_complete loss=0.9441 policy_loss=-0.0013 value_loss=1.9190 entropy=2.8300 clip_frac=0.009 kl=0.00143 explained_var=0.431
2025-04-18 12:18:58,685 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=90 reward=11.09 avg_reward=10.03 steps=15736 episode_steps=200 time=1144.02s
2025-04-18 12:19:46,109 | INFO | update_complete loss=0.5997 policy_loss=-0.0026 value_loss=1.2330 entropy=2.8274 clip_frac=0.018 kl=0.00207 explained_var=0.557
2025-04-18 12:21:08,924 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=100 reward=8.07 avg_reward=8.64 steps=17584 episode_steps=200 time=1274.26s
2025-04-18 12:21:08,924 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Evaluating at episode 100...
2025-04-18 12:22:15,376 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] eval episode=100 reward=36.45 avg_reward=20.48 time=1340.71s
2025-04-18 12:22:15,380 | INFO | model_saved path=artifacts\highway-ppo\ppo_highway_best_feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32.pth
2025-04-18 12:22:15,380 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] New best model saved with average reward: 20.48
2025-04-18 12:23:14,861 | INFO | update_complete loss=0.7140 policy_loss=-0.0005 value_loss=1.4572 entropy=2.8247 clip_frac=0.006 kl=0.00108 explained_var=0.597
2025-04-18 12:24:25,051 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=110 reward=7.93 avg_reward=16.17 steps=19432 episode_steps=200 time=1470.38s
2025-04-18 12:25:38,442 | INFO | update_complete loss=0.6367 policy_loss=-0.0013 value_loss=1.3042 entropy=2.8269 clip_frac=0.015 kl=0.00228 explained_var=0.669
2025-04-18 12:26:31,019 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=120 reward=4.50 avg_reward=14.78 steps=21280 episode_steps=200 time=1596.35s
2025-04-18 12:28:01,354 | INFO | update_complete loss=0.7445 policy_loss=-0.0046 value_loss=1.5264 entropy=2.8301 clip_frac=0.036 kl=0.00328 explained_var=0.704
2025-04-18 12:28:44,696 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=130 reward=3.06 avg_reward=12.16 steps=23128 episode_steps=200 time=1730.03s
2025-04-18 12:30:24,834 | INFO | update_complete loss=0.9996 policy_loss=-0.0010 value_loss=2.0295 entropy=2.8304 clip_frac=0.004 kl=0.00102 explained_var=0.658
2025-04-18 12:30:52,173 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=140 reward=22.23 avg_reward=16.12 steps=24976 episode_steps=200 time=1857.51s
2025-04-18 12:32:49,742 | INFO | update_complete loss=0.9335 policy_loss=-0.0056 value_loss=1.9065 entropy=2.8300 clip_frac=0.028 kl=0.00282 explained_var=0.761
2025-04-18 12:33:02,339 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=150 reward=20.79 avg_reward=15.48 steps=26824 episode_steps=200 time=1987.67s
2025-04-18 12:33:02,339 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] Evaluating at episode 150...
2025-04-18 12:34:06,638 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] eval episode=150 reward=56.38 avg_reward=29.46 time=2051.97s
2025-04-18 12:34:06,642 | INFO | model_saved path=artifacts\highway-ppo\ppo_highway_best_feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32.pth
2025-04-18 12:34:06,642 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] New best model saved with average reward: 29.46
2025-04-18 12:36:14,862 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=160 reward=7.96 avg_reward=23.26 steps=28672 episode_steps=48 time=2180.20s
2025-04-18 12:36:16,787 | INFO | update_complete loss=1.5101 policy_loss=-0.0053 value_loss=3.0592 entropy=2.8292 clip_frac=0.045 kl=0.00447 explained_var=0.709
2025-04-18 12:38:30,299 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=170 reward=35.56 avg_reward=25.85 steps=30672 episode_steps=200 time=2315.63s
2025-04-18 12:38:35,415 | INFO | update_complete loss=1.5823 policy_loss=-0.0017 value_loss=3.1963 entropy=2.8290 clip_frac=0.022 kl=0.00227 explained_var=0.781
2025-04-18 12:40:38,951 | INFO | [feat=presence,x,y,vx,vy_epochs=4_lr=0.0001_hidden_dim=64_batch_size=32] episode=180 reward=0.67 avg_reward=22.78 steps=32520 episode_steps=200 time=2444.28s
2025-04-18 12:40:56,507 | INFO | update_complete loss=1.6651 policy_loss=-0.0002 value_loss=3.3588 entropy=2.8295 clip_frac=0.011 kl=0.00148 explained_var=0.756
